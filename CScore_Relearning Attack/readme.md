# Tamper-Resistant Unlearning? â€” Relearning Attacks on Machine Unlearning Methods

Reproducing and extending the core empirical finding from:

**"From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization"**  
[arXiv:2505.22310](https://arxiv.org/abs/2505.22310)

**TL;DR**: Most popular approximate unlearning methods on CIFAR-100 (Gradient Ascent, SCRUB, NegGrad+, Random Relabelling, etc.) appear to forget target examples â€” until you fine-tune the "unlearned" model **only on the retain set**. Forget-set accuracy often recovers dramatically (80â€“100%), showing the knowledge was never truly removed.

## ðŸŽ¯ Goal of this repository

Demonstrate â€” in a clean, reproducible way â€” that many state-of-the-art unlearning algorithms are highly vulnerable to a simple **retain-set-only relearning attack**, even when using a principled subset selection strategy (lowest C-Score samples of a target class).

## ðŸ“Š Main results (CIFAR-100, ResNet-18)

| Method              | Forget Acc (after unlearning) | Forget Acc (after retain-only relearning) | Retain Acc (final) | Test Acc (final) | Notes                              |
|---------------------|-------------------------------|--------------------------------------------|--------------------|------------------|------------------------------------|
| No unlearning       | ~98%                          | â€”                                          | ~99.97%            | ~69.5%           | Original trained model             |
| Gradient Ascent     | ~2â€“10%                        | ~94â€“100%                                   | ~96â€“99%            | ~65â€“69%          | Very strong recovery               |
| BadTeacher          | ~0â€“5%                         | ~90â€“100%                                   | ~97â€“99%            | ~66â€“69%          | Similar vulnerability              |
| SCRUB               | ~10â€“20%                       | ~80â€“96%                                    | ~89â€“92%            | ~71â€“72%          | Moderate recovery                  |
| NegGrad+ (Î»=100)    | ~0%                           | ~88â€“96%                                    | ~98â€“99%            | ~68â€“69%          | Retain stays very strong           |
| Random Relabelling  | ~66%                          | ~98â€“100%                                   | ~95%               | ~58â€“69%          | Still highly recoverable           |

â†’ Conclusion: Standard unlearning metrics (forget acc right after unlearning) are misleading. Retain-only fine-tuning is a strong practical attack.

## âœ¨ Features of this implementation

- CScore-based forgetting: removes the **lowest-confidence 10%** of apple images (most "atypical" examples)
- Clean modular evaluation loop for relearning attacks
- Multiple popular unlearning baselines implemented and attacked
- Checkpoint saving for original + unlearned models
- Simple, self-contained notebook (`CScoreRelearningAttack.ipynb`)

