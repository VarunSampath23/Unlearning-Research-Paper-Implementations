{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff91c74-1a2e-442a-8995-6d3de9637ab3",
   "metadata": {},
   "source": [
    "## Contrastive Unlearning by Hong Kyu Lee et al at IJCAI 2025\n",
    "\n",
    "Paper link - https://arxiv.org/pdf/2401.10458\n",
    "1. They propose contrastive learning inspired unlearning paradigm. which utilises the concept of anchor, poisitive and negative samples.\n",
    "2. They show perfect forget accuracy and good retain accuracy on the emprical results.\n",
    "3. This works for both class level and sample level unlearning\n",
    "\n",
    "#### Lacks:\n",
    "1. Broader testing of model types, study focuses only on ResNet architecture family.\n",
    "2. Limited variety of datasets, as the authors have trained on only CIFAR10 and SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9aed7837-9d3c-421a-91ba-6502b442ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision torch scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a62032-dd1f-4c6f-b54f-f624fecf5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d46c33a-d778-40a8-8c18-3dea49ec9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        backbone = resnet18(pretrained=True)\n",
    "        \n",
    "        # Remove the final FC layer\n",
    "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])  # Output: (B, 512, 1, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)  # 512 is the output channels of last conv block\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.feature_extractor(x)           # (B, 512, 1, 1)\n",
    "        feats = feats.view(feats.size(0), -1)       # Flatten to (B, 512)\n",
    "        norm_feats = F.normalize(feats, p=2, dim=1) # L2 normalize\n",
    "        logits = self.fc(norm_feats)                # Class scores\n",
    "\n",
    "        return logits, norm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738d0021-0cc9-4c62-ba49-a8ad4075671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:07<00:00, 23.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "### Hyperparameters and Dataset Loading\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 5  # Short for demo; increase for better accuracy\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "\n",
    "# Data transforms (standard for CIFAR10)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR10\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0f7dbd6-7e4f-40b3-b6d1-325ce0eb8eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3867\n",
      "Epoch 2, Loss: 0.9506\n",
      "Epoch 3, Loss: 0.8059\n",
      "Epoch 4, Loss: 0.7253\n",
      "Epoch 5, Loss: 0.6638\n"
     ]
    }
   ],
   "source": [
    "#Training the model on CIFAR10\n",
    "model = ResNet18(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, trainloader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs,feats = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4a9fcd0-11d6-49b4-8c3b-9a8108f576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)  # Pick class index with highest logit\n",
    "            correct += (preds == y).sum().item()  # Sum of correct predictions\n",
    "            total += y.size(0)  # Total number of examples\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "560ebf3d-2613-4c4a-b40a-4328bea1acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "def split_dataset_by_class(dataset, forget_class):\n",
    "    retain_indices = []\n",
    "    forget_indices = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        if label == forget_class:\n",
    "            forget_indices.append(idx)\n",
    "        else:\n",
    "            retain_indices.append(idx)\n",
    "\n",
    "    retain_subset = Subset(dataset, retain_indices)\n",
    "    forget_subset = Subset(dataset, forget_indices)\n",
    "    \n",
    "    return retain_subset, forget_subset\n",
    "\n",
    "forget_class = 1\n",
    "\n",
    "# Use this for both train and test datasets\n",
    "retain_train_ds, forget_train_ds = split_dataset_by_class(train_loader.dataset, forget_class)\n",
    "retain_test_ds, forget_test_ds = split_dataset_by_class(test_loader.dataset, forget_class)\n",
    "\n",
    "# Wrap them in DataLoaders\n",
    "retain_train_dl = DataLoader(retain_train_ds, batch_size=train_loader.batch_size, shuffle=True, num_workers=2)\n",
    "forget_train_dl = DataLoader(forget_train_ds, batch_size=train_loader.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "retain_test_dl = DataLoader(retain_test_ds, batch_size=test_loader.batch_size, shuffle=False, num_workers=2)\n",
    "forget_test_dl = DataLoader(forget_test_ds, batch_size=test_loader.batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfebda5b-b0de-464f-bd94-eb81c686eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model accuracy:\n",
      "Retain Train Accuracy: 78.43%\n",
      "Forget Train Accuracy: 88.56%\n",
      "Retain Test Accuracy: 77.62%\n",
      "Forget Test Accuracy: 89.60%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model accuracy:\")\n",
    "\n",
    "acc_retain_train = compute_acc(model, retain_train_dl, device)\n",
    "print(f\"Retain Train Accuracy: {acc_retain_train:.2%}\")\n",
    "\n",
    "acc_forget_train = compute_acc(model, forget_train_dl, device)\n",
    "print(f\"Forget Train Accuracy: {acc_forget_train:.2%}\")\n",
    "\n",
    "acc_retain_test = compute_acc(model, retain_test_dl, device)\n",
    "print(f\"Retain Test Accuracy: {acc_retain_test:.2%}\")\n",
    "\n",
    "acc_forget_test = compute_acc(model, forget_test_dl, device)\n",
    "print(f\"Forget Test Accuracy: {acc_forget_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9db7f6-630c-48ba-a7b8-badd24325769",
   "metadata": {},
   "source": [
    "## Contrastive unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25b6fb73-ecc9-4874-a03d-a839f6f4b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(forget_logits,retain_logits):\n",
    "    logits =   torch.matmul(forget_logits,retain_logits.T)\n",
    "    \n",
    "    logits = -logits.mean()\n",
    "\n",
    "    return logits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def contrastive_loss(forget_logits, retain_logits):\n",
    "    \n",
    "    logits = torch.matmul(forget_logits, retain_logits.T)  # [Bf, Br]\n",
    "    return -logits.mean()  # Minimize similarity\n",
    "\n",
    "def unlearn(model, forget_train_dl, retain_train_dl, device='cuda'):\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Convert retrain_train_dl into a list so we can randomly sample from it\n",
    "    retain_batches = list(retain_train_dl)\n",
    "\n",
    "    for forget_x, forget_y in forget_train_dl:\n",
    "        forget_x, forget_y = forget_x.to(device), forget_y.to(device)\n",
    "\n",
    "\n",
    "        sampled_batches = random.sample(retain_batches, k=4)\n",
    "\n",
    "        for retain_x, retain_y in sampled_batches:\n",
    "            retain_x, retain_y = retain_x.to(device), retain_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            forget_logits, forget_feats = model(forget_x)\n",
    "            retain_logits, retain_feats = model(retain_x)\n",
    "            \n",
    "            retain_ce_loss = criterion(retain_logits, retain_y)\n",
    "            contrastive = contrastive_loss(forget_feats, retain_feats)\n",
    "\n",
    "            final_loss  = 3*contrastive + retain_ce_loss\n",
    "            \n",
    "            # Backprop\n",
    "            final_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc1218a0-1d31-466e-9dbe-ea2a1f648f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for unlearning: 9.668107748031616\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start  = time.time()\n",
    "unlearn(model, forget_train_dl, retain_train_dl)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Time Taken for unlearning: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b92a0c26-c5bd-48c7-b439-82a5dde70225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model accuracy:\n",
      "Retain Train Accuracy: 79.54%\n",
      "Forget Train Accuracy: 0.00%\n",
      "Retain Test Accuracy: 78.90%\n",
      "Forget Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model accuracy:\")\n",
    "\n",
    "acc_retain_train = compute_acc(model, retain_train_dl, device)\n",
    "print(f\"Retain Train Accuracy: {acc_retain_train:.2%}\")\n",
    "\n",
    "acc_forget_train = compute_acc(model, forget_train_dl, device)\n",
    "print(f\"Forget Train Accuracy: {acc_forget_train:.2%}\")\n",
    "\n",
    "acc_retain_test = compute_acc(model, retain_test_dl, device)\n",
    "print(f\"Retain Test Accuracy: {acc_retain_test:.2%}\")\n",
    "\n",
    "acc_forget_test = compute_acc(model, forget_test_dl, device)\n",
    "print(f\"Forget Test Accuracy: {acc_forget_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31926525-475b-4688-b97f-f676818ccc80",
   "metadata": {},
   "source": [
    "## ðŸ” Unlearning Evaluation Summary\n",
    "\n",
    "### âœ… Model Accuracy Before Unlearning\n",
    "| Dataset           | Accuracy |\n",
    "|-------------------|----------|\n",
    "| Retain Train      | 78.43%   |\n",
    "| Forget Train      | 88.56%   |\n",
    "| Retain Test       | 77.62%   |\n",
    "| Forget Test       | 89.60%   |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Model Accuracy After Unlearning\n",
    "| Dataset           | Accuracy |\n",
    "|-------------------|----------|\n",
    "| Retain Train      | 79.54%   |\n",
    "| Forget Train      | **0.00%**   |\n",
    "| Retain Test       | 78.90%   |\n",
    "| Forget Test       | **0.00%**   |\n",
    "\n",
    "---\n",
    "\n",
    "### â±ï¸ Time Taken for Unlearning\n",
    "- **9.67 seconds**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Key Observations:\n",
    "- The model **completely forgot** the target class (`forget_class = 3`), as intended.\n",
    "- Accuracy on **retain data** was preserved or even slightly improved after unlearning.\n",
    "- The unlearning process was efficient (under 10 seconds).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
